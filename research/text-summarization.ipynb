{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import pipeline,set_seed\nfrom datasets import load_dataset, load_from_disk\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nimport pandas as pd\nfrom datasets import load_dataset, load_metric\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nfrom tqdm import tqdm\nimport torch\n\nnltk.download(\"punkt\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-25T10:38:43.483026Z","iopub.execute_input":"2024-03-25T10:38:43.483900Z","iopub.status.idle":"2024-03-25T10:39:08.161063Z","shell.execute_reply.started":"2024-03-25T10:38:43.483858Z","shell.execute_reply":"2024-03-25T10:39:08.160094Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-25 10:38:52.027864: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-25 10:38:52.027986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-25 10:38:52.186029: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:08.162660Z","iopub.execute_input":"2024-03-25T10:39:08.163272Z","iopub.status.idle":"2024-03-25T10:39:08.216484Z","shell.execute_reply.started":"2024-03-25T10:39:08.163246Z","shell.execute_reply":"2024-03-25T10:39:08.215368Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"model_ckpt = \"google/pegasus-cnn_dailymail\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:08.217682Z","iopub.execute_input":"2024-03-25T10:39:08.218000Z","iopub.status.idle":"2024-03-25T10:39:32.480254Z","shell.execute_reply.started":"2024-03-25T10:39:08.217975Z","shell.execute_reply":"2024-03-25T10:39:32.479459Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3e8014cd264438696168cc7e966dfc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6432fbcce49c48dabc9a2eafbe1c24f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1adb035811d34373be0d272273ce28d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f3e692d21ee42179a2934fb31ab8155"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"106ffb6f788b453f84299953a96a0507"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fa7cc61e42d4c81943c92209207abdc"}},"metadata":{}}]},{"cell_type":"code","source":"!wget https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n!unzip summarizer-data.zip\n","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:32.482528Z","iopub.execute_input":"2024-03-25T10:39:32.482829Z","iopub.status.idle":"2024-03-25T10:39:35.279496Z","shell.execute_reply.started":"2024-03-25T10:39:32.482804Z","shell.execute_reply":"2024-03-25T10:39:35.278508Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2024-03-25 10:39:33--  https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\nResolving github.com (github.com)... 140.82.112.3\nConnecting to github.com (github.com)|140.82.112.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/summarizer-data.zip [following]\n--2024-03-25 10:39:33--  https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/summarizer-data.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7903594 (7.5M) [application/zip]\nSaving to: 'summarizer-data.zip'\n\nsummarizer-data.zip 100%[===================>]   7.54M  --.-KB/s    in 0.09s   \n\n2024-03-25 10:39:33 (84.6 MB/s) - 'summarizer-data.zip' saved [7903594/7903594]\n\nArchive:  summarizer-data.zip\n  inflating: samsum-test.csv         \n  inflating: samsum-train.csv        \n  inflating: samsum-validation.csv   \n   creating: samsum_dataset/\n extracting: samsum_dataset/dataset_dict.json  \n   creating: samsum_dataset/test/\n  inflating: samsum_dataset/test/data-00000-of-00001.arrow  \n  inflating: samsum_dataset/test/dataset_info.json  \n  inflating: samsum_dataset/test/state.json  \n   creating: samsum_dataset/train/\n  inflating: samsum_dataset/train/data-00000-of-00001.arrow  \n  inflating: samsum_dataset/train/dataset_info.json  \n  inflating: samsum_dataset/train/state.json  \n   creating: samsum_dataset/validation/\n  inflating: samsum_dataset/validation/data-00000-of-00001.arrow  \n  inflating: samsum_dataset/validation/dataset_info.json  \n  inflating: samsum_dataset/validation/state.json  \n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_samsum = load_from_disk(\"samsum_dataset\")\ndataset_samsum","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:35.280881Z","iopub.execute_input":"2024-03-25T10:39:35.281196Z","iopub.status.idle":"2024-03-25T10:39:35.319279Z","shell.execute_reply.started":"2024-03-25T10:39:35.281166Z","shell.execute_reply":"2024-03-25T10:39:35.318378Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n\nprint(f\"SPlit lenghts : {split_lengths}\")\nprint(f\"Features : {dataset_samsum['train'].column_names}\")\n\nprint(\"\\nDialogue :\")\n\nprint(dataset_samsum[\"test\"][1][\"dialogue\"])\nprint(\"\\n Summary\")\n\nprint(dataset_samsum[\"test\"][1][\"summary\"])","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:35.320431Z","iopub.execute_input":"2024-03-25T10:39:35.320704Z","iopub.status.idle":"2024-03-25T10:39:35.329685Z","shell.execute_reply.started":"2024-03-25T10:39:35.320681Z","shell.execute_reply":"2024-03-25T10:39:35.328876Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"SPlit lenghts : [14732, 819, 818]\nFeatures : ['id', 'dialogue', 'summary']\n\nDialogue :\nEric: MACHINE!\nRob: That's so gr8!\nEric: I know! And shows how Americans see Russian ;)\nRob: And it's really funny!\nEric: I know! I especially like the train part!\nRob: Hahaha! No one talks to the machine like that!\nEric: Is this his only stand-up?\nRob: Idk. I'll check.\nEric: Sure.\nRob: Turns out no! There are some of his stand-ups on youtube.\nEric: Gr8! I'll watch them now!\nRob: Me too!\nEric: MACHINE!\nRob: MACHINE!\nEric: TTYL?\nRob: Sure :)\n\n Summary\nEric and Rob are going to watch a stand-up on youtube.\n","output_type":"stream"}]},{"cell_type":"code","source":"def convert_examples_to_features(example_batch):\n    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n    \n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n        \n    return {\n        'input_ids' : input_encodings['input_ids'],\n        'attention_mask': input_encodings['attention_mask'],\n        'labels': target_encodings['input_ids']\n    }\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:35.330939Z","iopub.execute_input":"2024-03-25T10:39:35.331253Z","iopub.status.idle":"2024-03-25T10:39:35.339829Z","shell.execute_reply.started":"2024-03-25T10:39:35.331218Z","shell.execute_reply":"2024-03-25T10:39:35.338881Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)\n     ","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:35.341075Z","iopub.execute_input":"2024-03-25T10:39:35.341394Z","iopub.status.idle":"2024-03-25T10:39:41.003691Z","shell.execute_reply.started":"2024-03-25T10:39:35.341364Z","shell.execute_reply":"2024-03-25T10:39:41.002846Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cfe8f5a08b94b739247f7a55468ff95"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdbb8cc224ec4c87aff149771f13b46d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8ab65b957924085b491811e5cc4bf74"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_samsum_pt[\"train\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:41.004843Z","iopub.execute_input":"2024-03-25T10:39:41.005137Z","iopub.status.idle":"2024-03-25T10:39:41.011030Z","shell.execute_reply.started":"2024-03-25T10:39:41.005111Z","shell.execute_reply":"2024-03-25T10:39:41.010164Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 14732\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset_samsum_pt[\"train\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:41.013694Z","iopub.execute_input":"2024-03-25T10:39:41.013990Z","iopub.status.idle":"2024-03-25T10:39:41.065571Z","shell.execute_reply.started":"2024-03-25T10:39:41.013965Z","shell.execute_reply":"2024-03-25T10:39:41.064671Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 14732\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Training\n\nfrom transformers import DataCollatorForSeq2Seq\n\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:41.066675Z","iopub.execute_input":"2024-03-25T10:39:41.066944Z","iopub.status.idle":"2024-03-25T10:39:41.075720Z","shell.execute_reply.started":"2024-03-25T10:39:41.066911Z","shell.execute_reply":"2024-03-25T10:39:41.074935Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntrainer_args = TrainingArguments(\n    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n    weight_decay=0.01, logging_steps=10,\n    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n    gradient_accumulation_steps=16\n) ","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:41.076814Z","iopub.execute_input":"2024-03-25T10:39:41.077106Z","iopub.status.idle":"2024-03-25T10:39:41.107336Z","shell.execute_reply.started":"2024-03-25T10:39:41.077083Z","shell.execute_reply":"2024-03-25T10:39:41.106644Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model=model_pegasus, args=trainer_args,\n                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n                  train_dataset=dataset_samsum_pt[\"test\"], \n                  eval_dataset=dataset_samsum_pt[\"validation\"])","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:41.108510Z","iopub.execute_input":"2024-03-25T10:39:41.108776Z","iopub.status.idle":"2024-03-25T10:39:41.843484Z","shell.execute_reply.started":"2024-03-25T10:39:41.108754Z","shell.execute_reply":"2024-03-25T10:39:41.842697Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:39:41.844649Z","iopub.execute_input":"2024-03-25T10:39:41.844946Z","iopub.status.idle":"2024-03-25T10:44:37.155259Z","shell.execute_reply.started":"2024-03-25T10:39:41.844920Z","shell.execute_reply":"2024-03-25T10:44:37.154313Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ··············································································································································································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 174\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240325_103958-l4ittp7p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ai-software/huggingface/runs/l4ittp7p' target=\"_blank\">decent-deluge-1</a></strong> to <a href='https://wandb.ai/ai-software/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ai-software/huggingface' target=\"_blank\">https://wandb.ai/ai-software/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ai-software/huggingface/runs/l4ittp7p' target=\"_blank\">https://wandb.ai/ai-software/huggingface/runs/l4ittp7p</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 03:51, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=25, training_loss=3.162757759094238, metrics={'train_runtime': 294.9448, 'train_samples_per_second': 2.777, 'train_steps_per_second': 0.085, 'total_flos': 423944250507264.0, 'train_loss': 3.162757759094238, 'epoch': 0.98})"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluation\n\ndef generate_batch_sized_chunks(list_of_elements, batch_size):\n    \"\"\"split the dataset into smaller batches that we can process simultaneously\n    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i : i + batch_size]\n\n\n\ndef calculate_metric_on_test_ds(dataset, metric, model, tokenizer, \n                               batch_size=16, device=device, \n                               column_text=\"article\", \n                               column_summary=\"highlights\"):\n    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n\n    for article_batch, target_batch in tqdm(\n        zip(article_batches, target_batches), total=len(article_batches)):\n        \n        inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n                        padding=\"max_length\", return_tensors=\"pt\")\n        \n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                         attention_mask=inputs[\"attention_mask\"].to(device), \n                         length_penalty=0.8, num_beams=8, max_length=128)\n        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n        \n        # Finally, we decode the generated texts, \n        # replace the  token, and add the decoded texts with the references to the metric.\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n                                clean_up_tokenization_spaces=True) \n               for s in summaries]      \n        \n        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n        \n        \n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n        \n    #  Finally compute and return the ROUGE scores.\n    score = metric.compute()\n    return score\n\n     ","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:44:37.156845Z","iopub.execute_input":"2024-03-25T10:44:37.157188Z","iopub.status.idle":"2024-03-25T10:44:37.173135Z","shell.execute_reply.started":"2024-03-25T10:44:37.157154Z","shell.execute_reply":"2024-03-25T10:44:37.172175Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pip install rouge_score\n","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:49:27.452201Z","iopub.execute_input":"2024-03-25T10:49:27.452886Z","iopub.status.idle":"2024-03-25T10:49:42.693177Z","shell.execute_reply.started":"2024-03-25T10:49:27.452853Z","shell.execute_reply":"2024-03-25T10:49:42.692046Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=0626c251fc44750fe5ce653ef020f292d1d5510ff76f53dd7974d46030f1211f\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\nrouge_metric = load_metric('rouge')\n     ","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:49:42.695771Z","iopub.execute_input":"2024-03-25T10:49:42.696197Z","iopub.status.idle":"2024-03-25T10:49:42.929359Z","shell.execute_reply.started":"2024-03-25T10:49:42.696151Z","shell.execute_reply":"2024-03-25T10:49:42.928233Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\nscore = calculate_metric_on_test_ds(\n    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n)\n\nrouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n\npd.DataFrame(rouge_dict, index = [f'pegasus'] )","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:52:12.512887Z","iopub.execute_input":"2024-03-25T10:52:12.513499Z","iopub.status.idle":"2024-03-25T10:52:28.653746Z","shell.execute_reply.started":"2024-03-25T10:52:12.513468Z","shell.execute_reply":"2024-03-25T10:52:28.652583Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|██████████| 5/5 [00:15<00:00,  3.16s/it]\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"          rouge1  rouge2    rougeL  rougeLsum\npegasus  0.01713     0.0  0.016831   0.016879","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>pegasus</th>\n      <td>0.01713</td>\n      <td>0.0</td>\n      <td>0.016831</td>\n      <td>0.016879</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# save model \nmodel_pegasus.save_pretrained(\"pegasus-samsum-model\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:53:07.867375Z","iopub.execute_input":"2024-03-25T10:53:07.868268Z","iopub.status.idle":"2024-03-25T10:53:12.002532Z","shell.execute_reply.started":"2024-03-25T10:53:07.868235Z","shell.execute_reply":"2024-03-25T10:53:12.001500Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"# save tokenizer\ntokenizer.save_pretrained(\"tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-03-25T10:53:43.147659Z","iopub.execute_input":"2024-03-25T10:53:43.148041Z","iopub.status.idle":"2024-03-25T10:53:43.195821Z","shell.execute_reply.started":"2024-03-25T10:53:43.148013Z","shell.execute_reply":"2024-03-25T10:53:43.194724Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"('tokenizer/tokenizer_config.json',\n 'tokenizer/special_tokens_map.json',\n 'tokenizer/spiece.model',\n 'tokenizer/added_tokens.json',\n 'tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Replace '/path/to/tokenizer/directory' with the actual path to the tokenizer directory on your local machine\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/content/tokenizer\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-25T11:02:03.248050Z","iopub.execute_input":"2024-03-25T11:02:03.248624Z","iopub.status.idle":"2024-03-25T11:02:03.435964Z","shell.execute_reply.started":"2024-03-25T11:02:03.248587Z","shell.execute_reply":"2024-03-25T11:02:03.434219Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/working/content/tokenizer'. Use `repo_type` argument if needed.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Replace '/path/to/tokenizer/directory' with the actual path to the tokenizer directory on your local machine\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/content/tokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:767\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    769\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:600\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    599\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 600\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    617\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:462\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/kaggle/working/content/tokenizer'. Please provide either the path to a local folder or the repo_id of a model on the Hub."],"ename":"OSError","evalue":"Incorrect path_or_model_id: '/kaggle/working/content/tokenizer'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","output_type":"error"}]},{"cell_type":"code","source":"\n#Prediction\n\ngen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n\n\n\nsample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n\nreference = dataset_samsum[\"test\"][0][\"summary\"]\n\npipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n\n## \nprint(\"Dialogue:\")\nprint(sample_text)\n\n\nprint(\"\\nReference Summary:\")\nprint(reference)\n\n\nprint(\"\\nModel Summary:\")\nprint(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-03-25T11:02:19.533346Z","iopub.execute_input":"2024-03-25T11:02:19.533730Z","iopub.status.idle":"2024-03-25T11:02:42.332973Z","shell.execute_reply.started":"2024-03-25T11:02:19.533700Z","shell.execute_reply":"2024-03-25T11:02:42.331087Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n","output_type":"stream"},{"name":"stdout","text":"Dialogue:\nHannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him 🙂\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\n\nReference Summary:\nHannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n\nModel Summary:\nAmanda: Ask Larry Amanda: He called her last time we were at the park together .<n>Hannah: I'd rather you texted him .<n>Amanda: Just text him .\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}